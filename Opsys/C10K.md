**C10K问题由来**

随着互联网的普及，应用的用户群体几何倍增长，此时服务器性能问题就出现。最初的服务器是基于进程/线程模型。新到来一个TCP连接，就需要分配一个进程。假如有C10K，就需要创建1W个进程，可想而知单机是无法承受的。那么如何突破单机性能是高性能网络编程必须要面对的问题，进而这些局限和问题就统称为C10K问题，最早是由Dan Kegel进行归纳和总结的，并且他也系统的分析和提出解决方案。

**C10K问题的本质**

C10K问题的本质上是操作系统的问题。对于Web 1.0/2.0时代的操作系统，传统的同步阻塞I/O模型处理方式都是requests per second。当创建的进程或线程多了，数据拷贝频繁（缓存I/O、内核将数据拷贝到用户进程空间、阻塞，进程/线程上下文切换消耗大， 导致操作系统崩溃，这就是C10K问题的本质。

可见, 解决C10K问题的关键就是尽可能减少这些CPU资源消耗。



# C10K问题的解决方案

从网络编程技术的角度来说，主要思路：

1. 每个连接分配一个独立的线程/进程
2. 同一个线程/进程同时处理多个连接

**每个进程/线程处理一个连接**

该思路最为直接，但是申请进程/线程是需要系统资源的，且系统需要管理这些进程/线程，所以会使资源占用过多，可扩展性差

**每个进程/线程同时处理 多个连接(I/O多路复用)**

1. select方式：使用fd_set结构体告诉内核同时监控那些文件句柄，使用逐个排查方式去检查是否有文件句柄就绪或者超时。该方式有以下缺点：文件句柄数量是有上线的，逐个检查吞吐量低，每次调用都要重复初始化fd_set。
2. poll方式：该方式主要解决了select方式的2个缺点，文件句柄上限问题(链表方式存储)以及重复初始化问题(不同字段标注关注事件和发生事件)，但是逐个去检查文件句柄是否就绪的问题仍然没有解决。
3. epoll方式：该方式可以说是C10K问题的killer，他不去轮询监听所有文件句柄是否已经就绪。epoll只对发生变化的文件句柄感兴趣。其工作机制是，使用"事件"的就绪通知方式，通过epoll_ctl注册文件描述符fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd, epoll_wait便可以收到通知, 并通知应用程序。而且epoll使用一个文件描述符管理多个描述符,将用户进程的文件描述符的事件存放到内核的一个事件表中, 这样数据只需要从内核缓存空间拷贝一次到用户进程地址空间。而且epoll是通过内核与用户空间共享内存方式来实现事件就绪消息传递的，其效率非常高。但是epoll是依赖系统的(Linux)。



# epoll() 

#### 1.头文件

```c
#include <sys/epoll.h>
```

#### 2.函数原型

```c
int epoll_create(int size) //创建一个epoll实例，文件描述符
    
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) //将监听的文件描述符添加到epoll实例中，实例代码为将标准输入文件描述符添加到epoll中

int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) //等待epoll事件从epoll实例中发生， 并返回事件以及对应文件描述符
```

#### 3.参数类型

**`epoll_create(int size)`**

​	创建一个epoll的 句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll 后，必须调用close()关闭，否则可能导致fd被耗尽。

**`epoll_create1()`**

如果flags为0，epoll_create1（）和删除了过时size参数的epoll_create（）相同。

如果flags中包含以下值就有不同的表现：

EPOLL_CLOEXEC

在文件描述符上面设置执行时关闭（FD_CLOEXEC）标志描述符。

**`int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)`**

​	epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。

- 第一个参数是epoll_create()的返回值
- 第二个参数表示动作，用三个宏来表示：

```c
EPOLL_CTL_ADD //注册新的fd到epfd中；

EPOLL_CTL_MOD //修改已经注册的fd的监听事件；

EPOLL_CTL_DEL //从epfd中删除一个fd；
```

- 第三个参数是需要监听的fd
- 第四个参数是告诉内核需要监听什么事件

struct epoll_event结构体

```c
typedef union epoll_data {
    void *ptr;
    int fd;
    __uint32_t u32;
    __uint64_t u64;
} epoll_data_t;

struct epoll_event {
    __uint32_t events; /* Epoll events */
    epoll_data_t data; /* User data variable */
};
```

events

```c
EPOLLIN //表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；

EPOLLOUT //表示对应的文件描述符可以写；

EPOLLPRI //表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；

EPOLLERR //表示对应的文件描述符发生错误；

EPOLLHUP //表示对应的文件描述符被挂断；

EPOLLET //将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的；

EPOLLONESHOT //只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里。
```

**`int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout)`**

- epfd:由epoll_create 生成的epoll专用的文件描述符；
- epoll_event:用于回传代处理事件的数组；
- maxevents:每次能处理的事件数；
- timeout:等待I/O事件发生的超时值（ms）；-1永不超时，直到有事件产生才触发，0立即返回。
  返回发生事件数。-1有错误。

#### 4.使用

![img](https://images2018.cnblogs.com/blog/1117865/201806/1117865-20180627154041789-1218569040.png)

```c
#include <sys/epoll.h>
#define MAX_EVENTS 10
#include "../common/common.h"
#include "../common/head.h"
#include "../common/tcp_server.h"
#define BUFFSIZE 512

int main(int argc, char **argv) {
	char buff[BUFFSIZE] = {0};
    struct epoll_event ev, events[MAX_EVENTS];
    int listen_sock, conn_sock, nfds, epollfd;
	if (argc != 2) exit(1);
	listen_sock = socket_create(atoi(argv[1]));
    /* Code to set up listening socket, 'listen_sock',
       (socket(), bind(), listen()) omitted */

    epollfd = epoll_create1(0);
    if (epollfd == -1) {
        perror("epoll_create1");
        exit(EXIT_FAILURE);
    }

    ev.events = EPOLLIN;
    ev.data.fd = listen_sock;
    if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
        perror("epoll_ctl: listen_sock");
        exit(EXIT_FAILURE);
    }
	ev.events = EPOLLIN;
	ev.data.fd = listen_sock;

	if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
		perror("epoll_ctl:");
		exit(0);
	}

    for (;;) {
        nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1);
        if (nfds == -1) {
            perror("epoll_wait");
            exit(EXIT_FAILURE);
        }

        for (int n = 0; n < nfds; ++n) {
            if (events[n].data.fd == listen_sock) {
                conn_sock = accept(listen_sock, NULL, NULL);
                if (conn_sock == -1) {
                    perror("accept");
                    exit(EXIT_FAILURE);
                }
                make_nonblock(conn_sock);
                ev.events = EPOLLIN | EPOLLET;
                ev.data.fd = conn_sock;
                if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, &ev) == -1) {
                    perror("epoll_ctl: conn_sock");
                    exit(EXIT_FAILURE);
                }
            } else {
                //do_use_fd(events[n].data.fd);
				if (events[n].events & (EPOLLIN | EPOLLHUP | EPOLLERR)) {
					memset(buff, 0, sizeof(buff));
					if (recv(events[n].data.fd, buff, BUFFSIZE, 0) > 0) {
						printf("recv: %s", buff);
						for (int i = 0; i < strlen(buff); i++) {
							if (buff[i] >= 'a' && buff[i] <= 'z') buff[i] -= 32;
						}
						send(events[n].data.fd, buff, strlen(buff), 0);
					} else {
						if (epoll_ctl(epollfd, EPOLL_CTL_DEL, events[n].data.fd, NULL) < 0) {
							perror("epoll_ctrl");
						}
						printf("Logout!\n");
						close(events[n].data.fd);
					}
				}
            }
        }
    }
	return 0;
}

```



## poll()

#### 1.头文件

```c
#include <poll.h>
```

#### 2.函数原型

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout)
```

#### 3.参数类型

**fds:**指向一个结构体数组的第0个元素的指针，每个数组元素都是一个struct pollfd结构，用于指定测试某个给定的fd的条件

```c
struct pollfd{
	int fd;			//文件描述符
	short events;	//等待的事件
	short revents;	//实际发生的事件
};
```

|    常量    | events | revents |           说明           |
| :--------: | :----: | :-----: | :----------------------: |
|   POLLIN   |   Y    |    Y    |  普通或优先级带数据可读  |
| POLLRDNORM |   Y    |    Y    |       普通数据可读       |
| POLLRDBAND |   Y    |    Y    |     优先级带数据可读     |
|  POLLPRI   |   Y    |    Y    |     高优先级数据可读     |
|  POLLOUT   |   Y    |    Y    |       普通数据可写       |
| POLLWRNORM |   Y    |    Y    |       普通数据可写       |
| POLLWRBAND |   Y    |    Y    |     优先级带数据可写     |
|  POLLERR   |        |    Y    |         发生错误         |
|  POLLHUP   |        |    Y    |         发生挂起         |
|  POLLNVAL  |        |    Y    | 描述字不是一个打开的文件 |

**events：**指定监测fd的事件（输入、输出、错误），每一个事件有多个取值

**revents：**revents 域是文件描述符的操作结果事件，内核在调用返回时设置这个域。events 域中请求的任何事件都可能在 revents 域中返回

**nfds：**用来指定第一个参数数组元素个数

**timeout：**指定等待的毫秒数，无论 I/O 是否准备好，poll() 都会返回



| timeout值 | 说明                      |
| --------- | ------------------------- |
| -1        | 永远等待， 知道时间发生   |
| 0         | 立即返回                  |
| >0        | 等待指定timeout时间后返回 |

#### 4.返回值

成功时，poll() 返回结构体中 revents 域不为 0 的文件描述符个数；如果在超时前没有任何事件发生，poll()返回 0；

失败时，poll() 返回 -1，并设置 errno 为下列值之一：

EBADF：一个或多个结构体中指定的文件描述符无效。
EFAULT：fds 指针指向的地址超出进程的地址空间。
EINTR：请求的事件之前产生一个信号，调用可以重新发起。
EINVAL：nfds 参数超出 PLIMIT_NOFILE 值。
ENOMEM：可用内存不足，无法完成请求。



#### 5.原理

如果当前不可读，那么在sys_poll->do_poll中当前进程就会睡眠在等待队列上，这个等待队列是由驱动程序提供的（就是poll_wait中传入的那个）。当可读的时候，驱动程序可能有一部分代码运行了（比如驱动的中断服务程序），那么在这部分代码中，就会唤醒等待队列上的进程，也就是之前睡眠的那个，当那个进程被唤醒后do_poll会再一次的调用驱动程序的poll函数，这个时候应用程序就知道是可读的了。



## select()

#### 1.头文件

```c
#include <sys/select.h>
```

#### 2.函数原型

```c
int select(int nfdp, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```



```c
void FD_CLR(int fd,fd_set* set) //用来清除描述词组set中相关fd的位

int FD_ISSET(int fd,fd_set* set) // 用来测试描述词组set中相关fd的位是否为真

void FD_SET（int fd,fd_set* set）// 用来设置描述词组set中相关fd的位

void FD_ZERO（fd_set* set）// 用来清除描述词组set的全部位
```

#### 3.参数类型

**timeout**

timeout表示select返回之前的时间上限。

如果timeout==NULL，无期限等待下去，将select置于阻塞状态，一定等到监视文件描述符集合中某个文件描述符发生变化为止。如果是捕获到信号，select返回-1，并将变量errno设置成EINTR。

如果timeout->tv_sec == 0 && timeout->tv_sec == 0 ，不等待直接返回，纯粹的非阻塞函数，不管文件描述符是否有变化，都立刻返回继续执行，文件无变化返回0，有变化返回一个正值。

如果timeout->tv_sec！=0 || timeout->tv_sec！=0 ，等待指定的时间。当有描述符复合条件或者超过超时时间的话，函数返回。

timeval结构体

```c
struct timeval {
    long tv_sec; //秒
    long tv_usec; //微秒
};
```



#### 4.返回值

成功时：返回三中描述符集合中”准备好了“的文件描述符数量。
超时：返回0
错误：返回-1，并设置 errno



#### 5.原理

- select睡眠过程

select会循环遍历它所监测的fd_set（一组文件描述符(fd)的集合）内的所有文件描述符对应的驱动程序的poll函数。驱动程序提供的poll函数首先会将调用select的用户进程插入到该设备驱动对应资源的等待队列(如读/写等待队列)，然后返回一个bitmask告诉select当前资源哪些可用。当select循环遍历完所有fd_set内指定的文件描述符对应的poll函数后，如果没有一个资源可用(即没有一个文件可供操作)，则select让该进程睡眠，一直等到有资源可用为止，进程被唤醒(或者timeout)继续往下执行。

select的调用path如下：sys_select -> core_sys_select -> do_select

- select唤醒过程

select会循环遍历它所监测的fd_set内的所有文件描述符对应的驱动程序的poll函数。驱动程序提供的poll函数首先会将调用select的用户进程插入到该设备驱动对应资源的等待队列(如读/写等待队列)，然后返回一个bitmask告诉select当前资源哪些可用。



#### 6.使用

伪代码

```c
fd_set readfds;
int fd;
FD_ZERO(&readfds)//新定义的变量要清空一下,相当于初始化。
FD_SET(fd,&readfds);//把文件描述符fd加入到readfds中。
//do select 返回
if(FD_ISSET(fd,&readset))//判断是否成功监视
{
    //dosomething
}
```



man手册使用例子

```c
#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>
#include <sys/types.h>
#include <unistd.h>


int main(void) {
    fd_set rfds;
    struct timeval tv;
    int retval;

    /* Watch stdin (fd 0) to see when it has input. */

    FD_ZERO(&rfds);
    FD_SET(0, &rfds);

    /* Wait up to five seconds. */

    tv.tv_sec = 5;
    tv.tv_usec = 0;

    retval = select(1, &rfds, NULL, NULL, &tv);
    /* Don't rely on the value of tv now! */

    if (retval == -1)
        perror("select()");
    else if (retval) {
  	  char buff[512] = {0};
        printf("Data is available now.\n");
  	  scanf("%s", buff);
    }
        /* FD_ISSET(0, &rfds) will be true. */
    else
        printf("No data within five seconds.\n");

    exit(EXIT_SUCCESS);
}

```

使用select完成TCP非堵塞链接

```c
#include "../common/color.h"
#include "../common/common.h"
#include "../common/tcp_server.h"
#include "../common/head.h"

#define	CLIENTSIZE 10
#define BUFFSIZE 512
char ch_char(char c) {
	if (c >= 'a' && c <= 'z')
		return c - 32;
	return c;
}


int main(int argc, char **argv) {
	if (argc != 2) {
		fprintf(stderr, "Usage: %s port!\n", argv[0]);
		exit(1);
	}
	int server_listen, fd, max_fd;
	int client[CLIENTSIZE] = {0};
	memset(client, -1, CLIENTSIZE);
	if ((server_listen = socket_create(atoi(argv[1]))) < 0) {
		perror("socket_create");
		exit(1);
	}

	make_nonblock(server_listen);
	
	fd_set rfds, wfds, efds;
	max_fd = server_listen;
    client[server_listen] = server_listen;

	while (1) {
		FD_ZERO(&rfds);
		FD_ZERO(&wfds);
		FD_ZERO(&efds);

		FD_SET(server_listen, &rfds);
		
		for (int i = 0; i < CLIENTSIZE; i++) {
			if (client[i] == server_listen) continue;
			if (client[i] > 0) {
				if (max_fd < client[i]) max_fd = client[i];
				FD_SET(client[i], &rfds);
			}
		}

		if (select(max_fd + 1, &rfds, NULL, NULL, NULL) < 0) {
			perror("select");
			return 1;
		}

		if (FD_ISSET(server_listen, &rfds)) {
			printf("Connect ready on serverlisten\n");
			if ((fd = accept(server_listen, NULL, NULL)) < 0) {
				perror("accept");
				return 1;
			}
			if (fd > CLIENTSIZE) {
				printf("too many clients\n");
				close(fd);
			} else {
				make_nonblock(fd);
				if (client[fd] == -1)
					client[fd] = fd;
			}
		}
		for (int i = 0; i < CLIENTSIZE; i++) {
			if (i == server_listen) continue;
			if (FD_ISSET(i, &rfds)) {
				char buff[BUFFSIZE] = {0};
				int retval = recv(i, buff, BUFFSIZE, 0);
				if (retval <= 0) {
					printf("logout!\n");
					client[i] = -1;
					close(i);
					continue;
				}
				printf("recv: %s", buff);
				for (int j = 0; j < strlen(buff); j++) {
					buff[j] = ch_char(buff[j]);
				}
				send(i, buff, strlen(buff), 0);
			}
		}
	}
	return 0;
}


```



# select poll epoll区别

## select 

**缺点**

- bitmap默认大小1024
- fd_set不可重用，每次都要重新使用FD_SET
- 用户态到内核态开销
- O(n)复杂度



## poll

制位的是pollfd.revents，可以重用

- 用户态到内核态开销
- O(n)复杂度

## epoll

- 有一部分用户态 内核态共享内存
- wait将触发事件的fd放在前面，复杂度为O(n)

`epoll`使用`RB-Tree`红黑树去监听并维护所有文件描述符，`RB-Tree`的根节点

调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个 **红黑树** 用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.

当**epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效**。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.

当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。

epoll相比于select并不是在所有情况下都要高效，例如在如果有少于1024个文件描述符监听，且大多数socket都是出于活跃繁忙的状态，这种情况下，select要比epoll更为高效，因为epoll会有更多次的系统调用，用户态和内核态会有更加频繁的切换。

epoll高效的本质在于：

- 减少了用户态和内核态的文件句柄拷贝
- 减少了对可读可写文件句柄的遍历
- mmap 加速了内核与用户空间的信息传递，epoll是通过内核与用户mmap同一块内存，避免了无谓的内存拷贝
- IO性能不会随着监听的文件描述的数量增长而下降
- 使用红黑树存储fd，以及对应的回调函数，其插入，查找，删除的性能不错，相比于hash，不必预先分配很多的空间

